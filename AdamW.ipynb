{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class AdamW:\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.0):\n",
    "        self.params = params                  \n",
    "        self.lr = lr\n",
    "        self.beta1, self.beta2 = betas\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.t = 0\n",
    "        self.state = {}                     \n",
    "\n",
    "    def step(self, grads):\n",
    "        self.t += 1\n",
    "\n",
    "        for param, grad in zip(self.params, grads):\n",
    "            key = id(param)\n",
    "\n",
    "            if key not in self.state:\n",
    "               self.state[key] ={\n",
    "                   'm': np.zeros_like(param),\n",
    "                   'v': np.zeros_like(param)\n",
    "               }\n",
    "\n",
    "            m = self.state[key]['m']\n",
    "            v = self.state[key]['v']\n",
    "\n",
    "            m[:] =  m * self.beta1 + grad * (1 - self.beta1)\n",
    "            v[:] = v * self.beta2 + (grad ** 2) * (1 - self.beta2)\n",
    "\n",
    "            m_hat = m / (1 - self.beta1 ** self.t)\n",
    "            v_hat = v / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            param -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "\n",
    "            if self.weight_decay != 0:\n",
    "                if param.ndim > 1:\n",
    "                    param -= - self.lr * self.weight_decay * param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before AdamW update:  [array([ 1., -2.]), array([0.5])]\n",
      "After AdamW update:  [array([ 0.9       , -1.90000001]), array([0.4])]\n"
     ]
    }
   ],
   "source": [
    "# input parameters of one nn layer\n",
    "W = np.array([1.0, -2.0])\n",
    "b = np.array([0.5])\n",
    "\n",
    "params = [W, b]\n",
    "\n",
    "# gradients from backpropagation\n",
    "dW = np.array([0.3, -0.1])\n",
    "db = np.array([0.2])\n",
    "\n",
    "grads = [dW, db]\n",
    "\n",
    "print('Before AdamW update: ', params)\n",
    "\n",
    "opt = AdamW(params, lr=0.1, weight_decay=0.1)\n",
    "\n",
    "opt.step(grads)\n",
    "\n",
    "print('After AdamW update: ', params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
